import argparse
from audioop import reverse
from crypt import methods

import torch.distributed as dist
import torch.utils.data
from torch.utils.tensorboard import SummaryWriter
import pandas as pd
import operator
import numpy as np
import copy


from utils.utils import *


####### Pruning ########
from filter_pruning.search.search_filter_pruning import *
from filter_pruning.fp_utils.utils import *
import filter_pruning.torch_pruning as tp
from filter_pruning.torch_pruning.pruning import filter_pruning


# warmup = True
mixed_precision = True
try:  # Mixed precision training https://github.com/NVIDIA/apex
    from apex import amp
except:
    print('Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex')
    mixed_precision = False  # not installed

# Hyperparameters
hyp = {
       'optimizer': 'SGD',  # ['adam', 'SGD', None] if none, default is SGD
       'lr0': 0.001,  # initial learning rate (SGD=1E-2, Adam=1E-3)
        # 'optimizer': 'adam',  # ['adam', 'SGD', None] if none, default is SGD
        # 'lr0': 0.0001,  # initial learning rate (SGD=1E-2, Adam=1E-3)
       'momentum': 0.937,  # SGD momentum/Adam beta1
       'weight_decay': 5e-4,  # optimizer weight decay
       'giou': 0.05,  # giou loss gain
       'cls': 0.5,  # cls loss gain
       'cls_pw': 1.0,  # cls BCELoss positive_weight
       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)
       'obj_pw': 1.0,  # obj BCELoss positive_weight
       'iou_t': 0.20,  # iou training threshold
       'anchor_t': 4.0,  # anchor-multiple threshold
       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)
       'hsv_h': 0.015,  # image HSV-Hue augmentation (fraction)
       'hsv_s': 0.7,  # image HSV-Saturation augmentation (fraction)
       'hsv_v': 0.4,  # image HSV-Value augmentation (fraction)
       'degrees': 0.0,  # image rotation (+/- deg)
       'translate': 0.0,  # image translation (+/- fraction)
       'scale': 0.5,  # image scale (+/- gain)
       'shear': 0.0}  # image shear (+/- deg)

savelog = False



def train_filter_pruning(hyp, tb_writer, opt, device):

    print(f'Hyperparameters {hyp}')

    from filter_pruning.fp_train import Training
    trainer = Training(hyp, opt, device, mixed_precision=mixed_precision, savelog=False) 

    trainer.create_optimizer()
    trainer.load_model_weights()


    model_init, imgsz = trainer.get_model()

    #################################################
    # Torch Pruning (Begin)
    #################################################
    pruning_rate = [0.0,  # m.0
                    0.0,  # m.1
                    0.1,  # m.2
                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1,  # m.3  csp1_1 (cv1 cv2 cv3 cv4 m.0.cv1 m.0.cv2)
                    ## m.3 csp1_1 0.1
                    ##   cv1 0.2
                    ##   cv2 0.1
                    ##   m.0.cv1 m.0.cv2 cv3 0.3(cv1+m.0.cv2+itself)
                    ##   cv4 0.3
                    ##   m.4 0.1
                    0.3,  # m.4
                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,  # m.5  csp1_3  (cv1 cv2 cv3 cv4 m.0.cv1 m.0.cv2 m.1.cv1 m.1.cv2 m.2.cv1 m.2.cv2)
                    ## m.5 csp1_3 0.1
                    ##   cv1 0.4
                    ##   cv2 0.1
                    ##   m.0.cv1 m.0.cv2 m.1.cv1 m.1.cv2 m.2.cv1 m.2.cv2 cv3 0.5
                    ##   cv4 0.3
                    ##   m.6 m18.cv1 m18.cv2  0.1
                    0.4,  # m.6
                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,  # m.7  csp1_3
                    ## m.7 csp1_3 0.1
                    ##   cv1 0.4
                    ##   cv2 0.1
                    ##   m.0.cv1 m.0.cv2 m.1.cv1 m.1.cv2 m.2.cv1 m.2.cv2 cv3 0.5
                    ##   cv4 0.3
                    ##   m.8 m14.cv1 m14.cv2  0.1
                    0.4,  # m.8
                    0.3, 0.3,  # m.9  spp
                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1,  # m.10    csp2_1
                    0.0,   # m.11  upsample
                    0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # m.14    csp2_1
                    0.0,   # m.15  upsample
                    0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # m.18    csp2_1
                    0.0,   # m.19
                    0.0, 0.0, 0.0, 0.0, 0.0, 0.0,  # m.21    csp2_1
                    0.0,   # m.22
                    0.0, 0.0, 0.0, 0.0, 0.0, 0.0  # m.24
                    ]
    
    model = filter_pruning(model_init, imgsz, pruning_rate)
    model=model.to(device)
    ema_model, model, _, _ = trainer.train(setting_epochs=20, in_model=model, warmup=True)

    results = trainer.test(in_model=ema_model)
    results = trainer.test(in_model=model)
    
    # print('\nScore = {:.4f}\n'.format(float(results[2])), flush=True)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')
    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='data.yaml path')
    parser.add_argument('--hyp', type=str, default='', help='hyp.yaml path (optional)')
    parser.add_argument('--epochs', type=int, default=300)
    parser.add_argument('--batch-size', type=int, default=16, help="Total batch size for all gpus.")
    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')
    parser.add_argument('--rect', action='store_true', help='rectangular training')
    parser.add_argument('--resume', nargs='?', const='get_last', default=False,
                        help='resume from given path/to/last.pt, or most recent run if blank.')
    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')
    parser.add_argument('--notest', action='store_true', help='only test final epoch')
    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')
    parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')
    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')
    parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')
    parser.add_argument('--weights', type=str, default='', help='initial weights path')
    parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')
    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')
    parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')
    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')
    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')
    opt = parser.parse_args()



    ##########################
    ###### load weights ###### 
    ##########################
    # resume: default(False) 
    # if its true, load weights from latest run
    last = get_latest_run() if opt.resume == 'get_last' else opt.resume  # resume from most recent run
    if last and not opt.weights:
        print(f'Resuming training from {last}')
    opt.weights = last if opt.resume and not opt.weights else opt.weights
    
    ##########################################
    ####### check file/data,model, hyp  ###### 
    ##########################################
    # hyp: default=''
    # cfg: model
    # data: dataset
    # if opt.local_rank in [-1, 0]:
    #     check_git_status()
    opt.cfg = check_file(opt.cfg)  # check file
    opt.data = check_file(opt.data)  # check file
    if opt.hyp:  # update hyps
        opt.hyp = check_file(opt.hyp)  # check file
        with open(opt.hyp) as f:
            hyp.update(yaml.load(f, Loader=yaml.FullLoader))  # update hyps
    
    ###########################
    ### init some opt.param ### 
    ###########################
    # mixed_precision: true
    # local_rank: -1
    # DDP mode: distributed data parallel training
    opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)
    device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)
    opt.total_batch_size = opt.batch_size
    opt.world_size = 1
    if device.type == 'cpu':
        mixed_precision = False
    elif opt.local_rank != -1:
        # DDP mode
        assert torch.cuda.device_count() > opt.local_rank
        torch.cuda.set_device(opt.local_rank)
        device = torch.device("cuda", opt.local_rank)
        dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend

        opt.world_size = dist.get_world_size()
        assert opt.batch_size % opt.world_size == 0, "Batch size is not a multiple of the number of devices given!"
        opt.batch_size = opt.total_batch_size // opt.world_size
    print(opt)

    ##############################
    ### search filter pruning  ### 
    ##############################
    if opt.local_rank in [-1, 0]:
        if (savelog):
            print('Start Tensorboard with "tensorboard --logdir=runs", view at http://localhost:6006/')
            tb_writer = SummaryWriter(log_dir=increment_dir('runs/exp', opt.name))
        else:
            tb_writer = None
    else:
        tb_writer = None

    train_filter_pruning(hyp, tb_writer, opt, device)




# python train_pruning.py --cfg ./models/yolov5s_ori.yaml --data data/quexian.yaml --img-size 320 --weights weights/DET-biaopan_quexian-20211028.pt --batch-size 32 --epochs 10 --device 0